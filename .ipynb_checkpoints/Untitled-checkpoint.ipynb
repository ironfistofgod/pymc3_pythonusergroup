{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): install mkl with `conda install mkl-service`: No module named 'mkl'\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b06b2c49a66f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model() as model:\n",
    "    m = pm.Normal('m', 0, 1)\n",
    "    b = pm.Normal('b', -1, 10)\n",
    "    c = pm.Normal('c', 1, 10)\n",
    "    \n",
    "    trace = pm.sample(20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "4 chains in "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(trace)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdtrace['a'].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orered = pm.distributions.transforms.Ordered()\n",
    "testval = np.asarray([-1, 1, 4])\n",
    "with pm.Model() as m:\n",
    "    x = pm.Normal('x', 0, 1,\n",
    "                 shape=3,\n",
    "                 transform=ordered,\n",
    "                 testval=testval)\n",
    "    trace = pm.shape(50000)\n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "practice model one: from strat column, invert for relative sea level on that column of rock.  Or if we had a known sea level curve, could we invert for time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orered = pm.distributions.transforms.Ordered()\n",
    "testval = np.asarray([-1, 1, 4])\n",
    "with pm.Model() as m:\n",
    "    sea_level_from_observations = 2\n",
    "    age_of_observations = ??\n",
    "    sea_level_curve = function(t)\n",
    "    \n",
    "    x = pm.Normal('x', 0, 1,\n",
    "                 shape=3,\n",
    "                 transform=ordered,\n",
    "                 testval=testval)\n",
    "    trace = pm.shape(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "given observations, how do we get back to time? \n",
    "\n",
    "inference_model_real_data (from blake's coe\n",
    "\n",
    "shared = taking from numpy and transferring it into a graph\n",
    "\n",
    "\n",
    "highstand: prior on age is near end of interglacial, subtrat from 117 (or add to)\n",
    "set priors for GP parameters.  Mean function = pm.gp.mean.Constant\n",
    "\n",
    "create more complex covariance kernel\n",
    "Those go into gaussian process marginal function: gaussian process plus white noise.  Latent would be if you want to efine noise in another way.  \n",
    "\n",
    "for every coral in highstand \n",
    "ages = {}\n",
    "ages = []\n",
    "list of random vairables with ifferent priors.  \n",
    "deterministic: collect it to look at it in output.  Something determined by random simulations in priors.\n",
    "\n",
    "GIa = tt.set_subtensor(GIA[i], \n",
    "z-functions = gia models\n",
    "ages = simulated age, x pulled out of priors for each time through loop of sampler.  If age is 124 on this simulation, what is your GIA at that location?  In z_functions, for all datapoints, there's a function of RSL over time, an interpolation function -- pass that at any time and gives an output for what GIA is.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt = np.linspace(0, np.pi, 100)\n",
    "\n",
    "function = interp1d(xt, np.sin(xt))\n",
    "\n",
    "plt.plot(function(1,2,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In pymc3, pass interpolation function into model using splinewrapper.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orered = pm.distributions.transforms.Ordered()\n",
    "testval = np.asarray([-1, 1, 4])\n",
    "with pm.Model() as m:\n",
    "    sea_level_from_observations = 2\n",
    "    age_of_observations = ?? pm.Uniform(0,100)\n",
    "    sea_level_curve = function(t)\n",
    "    inference = function(age_of_observation, observed = 2)\n",
    "    \n",
    "    x = pm.Normal('x', 0, 1,\n",
    "                 shape=3,\n",
    "                 transform=ordered,\n",
    "                 testval=testval)\n",
    "    trace = pm.shape(50000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some functions, some observations, what is their age?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "splinewrapper -- input from pymc3.distributions.dist_math import SplineWrapper\n",
    "A function that wraps around univariate spline that pulls interpolation functioon into Theano.  \n",
    "\n",
    "interpolation_functions:\n",
    "    \n",
    "    UnivariateSpline(age, RSL, values for interpolation)\n",
    "    \n",
    "get posterior that's interpolated.  \n",
    "\n",
    "pass it 50 functions to find age -- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RSL at each data point at the random prior ages -- 121 plus/minus 2.  You'd get this oval-shaped RSL posterior.  Depending on data, add water depth.  If corals, distribution.  if index points, don't add anything.  The observed noise from GPS uncertainty is already in therefrom observations.  \n",
    "\n",
    "Set observed eleveaation, observed elev. uncertainty \n",
    "\n",
    "Add white noise on top \n",
    "\n",
    "Deterministic throws right side into dictionary on right sie.  \n",
    "\n",
    "once you've defined something like pm.Normal -- you can do normal python math.  If you run into python math, go to theano docs : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to work in ordered transforms  -- superposition to timing or bounded.  Those will be import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the goal from individual tasks?  For Jacky's task, don't see need for extra steps.  In task for specifically of that which we can't fit what might it be?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take GIA models: can we use weightings/likelihoods -- generate posterior from GIA models alone.  Weighting part is from data.  Is that different than what Kopp group is trying to do?  They use kernel from GIA models but is that different?  \n",
    "\n",
    "If fitting residual with GIA-like kernels, then residuals that is furthest from data -- a large residual, suddenly you have kernels that look like GIA.  REsidual that looks nothing like GIA -- very poor fits using tuned kernels.  Funny business there .  Map of likelihoods of likelihoods.  How much white noise .  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But then fitting kernel to residual, how much GIA is left?  Don't know really good way to deal with that.  Unless you use real model outputs to make kernels and never subtract real model from data.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
